{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Approaches to Defining Neural Networks with Keras and TensorFlow\n",
    "\n",
    "[TensorFlow](https://www.tensorflow.org/) 2.0 came with many new exciting updates. One of these updates was full integration with the very popular [Keras API](https://keras.io/) for developing deep learning models. Before TensorFlow 2.0, you had two install TensorfFlow and Keras separately. Now, Keras comes as a submodule of TensorFlow (*i.e.*, tensorflow.keras). We will be using Keras and TensorFlow on the majority of tutorials in this class. There are 3 ways to define Neural Networks with Keras. In this tutorial we will cover these different ways.\n",
    "\n",
    "The learning goals of this tutorial are:\n",
    "    - Introduce the Keras sequential API, functional API and model subclassing methods for defining neural networks;\n",
    "    - Illustrate a simple classiifcation problem using the Iris dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Python Libraries \n",
    "\n",
    "If you get an error that a library is not installed, most libraries you can stall on a jupyter notebook by creating a new cell and typing:\n",
    "\n",
    "- *! pip install library_name*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "import keras_cv \n",
    "import keras_cv\n",
    "from tensorflow.keras.utils import to_categorical # Function to convert labels to one-hot encoding\n",
    "import pandas as pd  \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.datasets import load_iris  # Function for loading the Iris dataset\n",
    "from sklearn.model_selection import train_test_split # Function for splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and return to the defined variable \n",
    "dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris data into a DataFrame\n",
    "dframe = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "# add \"target_label\" column to the dataset and name it \"label\"\n",
    "dframe['labels'] = dataset.target.astype(int) # Labels are represented as integers\n",
    "# use of String label\n",
    "dframe['label_names'] = dframe.labels.replace(dict(enumerate(dataset.target_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>labels</th>\n",
       "      <th>label_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   labels label_names  \n",
       "0       0      setosa  \n",
       "1       0      setosa  \n",
       "2       0      setosa  \n",
       "3       0      setosa  \n",
       "4       0      setosa  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prints the 5 first rows/samples of the dataset\n",
    "dframe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)      labels  \n",
       "count        150.000000  150.000000  \n",
       "mean           1.199333    1.000000  \n",
       "std            0.762238    0.819232  \n",
       "min            0.100000    0.000000  \n",
       "25%            0.300000    0.000000  \n",
       "50%            1.300000    1.000000  \n",
       "75%            1.800000    2.000000  \n",
       "max            2.500000    2.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generates a short description of the dataset (missing values, mean values, etc.)\n",
    "dframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation and Test Sets Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the features and labels from the dataset \n",
    "X = np.asarray(dframe[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']])\n",
    "Y = np.asarray(dframe['labels'])\n",
    "\n",
    "# First we will shuffle the samples\n",
    "indexes = np.arange(X.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "X = X[indexes,:]\n",
    "Y = Y[indexes]\n",
    "\n",
    "# Then, we split our data into train/val/test sets\n",
    "train_split = int(0.5*Y.size)\n",
    "val_split = int(0.75*Y.size)\n",
    "\n",
    "X_train = X[:train_split,:]\n",
    "Y_train = Y[:train_split]\n",
    "\n",
    "X_val = X[train_split:val_split,:]\n",
    "Y_val = Y[train_split:val_split]\n",
    "\n",
    "X_test = X[val_split:,:]\n",
    "Y_test = Y[val_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max data normalization\n",
    "x_train_min = X_train.min(axis = 0, keepdims = True)\n",
    "x_train_max = X_train.max(axis = 0, keepdims = True)\n",
    "\n",
    "train_size = int(num_samples * 0.8)\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "Y_train, Y_val = Y[:train_size], Y[train_size:]\n",
    "\n",
    "X_train = (X_train - x_train_min)/(x_train_max - x_train_min)\n",
    "X_val = (X_val - x_train_min)/(x_train_max - x_train_min)\n",
    "X_test = (X_test - x_train_min)/(x_train_max - x_train_min)\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "#Activity suggestion:\n",
    "# 1. Change the min-max normalization above by standardization ((X - mean)/(std))\n",
    "# 2. Don't normalize the data and see what happens\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing Labels using one-hot-ecoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m k \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(Y)\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m      2\u001b[0m Y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      3\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(Y)\u001b[38;5;241m.\u001b[39msize\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "k = np.unique(Y).size\n",
    "X = np.random.rand(num_samples, input_shape)  # Random features\n",
    "Y = np.random.randint(0, num_classes, num_samples)  # Random labels\n",
    "num_samples = 1000  # Total number of samples\n",
    "input_shape = 20    # Number of features for each sample\n",
    "num_classes = 3     # Number of unique classes\n",
    "Y_oh_train = to_categorical(Y_train, num_classes=num_classes)\n",
    "Y_oh_val = to_categorical(Y_val, num_classes=num_classes)\n",
    "Y_oh_train = to_categorical(Y_train, num_classes=3)  # 3 for 3 output classes\n",
    "Y_oh_val = to_categorical(Y_val, num_classes=3)\n",
    "Y_oh_test = to_categorical(Y_test, k)\n",
    "X_train, X_val = X[:train_size], X[train_size:]  # Split data\n",
    "y_train, y_val = y[:train_size], y[train_size:]\n",
    "# Displaying the 5 first elements\n",
    "print('Y_train[:5]:')\n",
    "print(Y_train[:5])\n",
    "print('\\nY_oh_train[:5]=')\n",
    "print(Y_oh_train[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_train :  (75, 4)\n",
      "Size of X_val :  (37, 4)\n",
      "Size of X_test :  (38, 4)\n"
     ]
    }
   ],
   "source": [
    "print( \"Size of X_train : \" , X_train.shape)\n",
    "print( \"Size of X_val : \" , X_val.shape)\n",
    "print( \"Size of X_test : \" , X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Approaches for Defining Neural Networks\n",
    "\n",
    "### 1. The Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, 5)                 25        \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, 10)                60        \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118\n",
      "Trainable params: 118\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "# Passing a list of layers to the constructor\n",
    "model = Sequential([\n",
    "    tf.keras.layers.Dense(5, activation='relu', input_shape=(4,) , name = \"layer1\"),\n",
    "    tf.keras.layers.Dense(10, activation='relu' , name = \"layer2\"),\n",
    "    tf.keras.layers.Dense(3, activation='softmax', name = \"layer3\"),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 25        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                60        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118\n",
      "Trainable params: 118\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "# This returns a tensor\n",
    "input_tensor = tf.keras.layers.Input(shape=(4,))\n",
    "# A layer instance is callable on a tensor, and returns a tensor\n",
    "x1 = tf.keras.layers.Dense(5, activation='relu')(input_tensor)\n",
    "x2 = tf.keras.layers.Dense(10, activation='relu')(x1)\n",
    "out_tensor = tf.keras.layers.Dense(3, activation='softmax')(x2)\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(inputs=input_tensor, outputs=out_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Subclassing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_neural_network\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             multiple                  25        \n",
      "                                                                 \n",
      " dense_4 (Dense)             multiple                  60        \n",
      "                                                                 \n",
      " dense_5 (Dense)             multiple                  33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118\n",
      "Trainable params: 118\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class MyNeuralNetwork(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyNeuralNetwork, self).__init__(**kwargs)\n",
    "        self.dense1 = tf.keras.layers.Dense(5, activation='relu', )\n",
    "        self.dense2 = tf.keras.layers.Dense(10, activation='relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(3, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x1 = self.dense1(inputs)\n",
    "        x2 = self.dense2(x1)\n",
    "        out_tensor = self.dense3(x2)\n",
    "        return out_tensor\n",
    "model = MyNeuralNetwork()\n",
    "model.build(input_shape = (None,4))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_cv  # Ensure you have keras_cv installed\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Define input shape and number of classes\n",
    "input_shape = 100  # Example input shape; replace with your actual input size\n",
    "num_classes = 10   # Example number of classes; replace with your actual number\n",
    "\n",
    "# Define the model with Input layer\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(input_shape,)),  # Define the input shape here\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define the focal loss\n",
    "focal_loss = keras_cv.losses.FocalLoss(alpha=0.25, gamma=2.0)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss=focal_loss, metrics=['accuracy'])\n",
    "\n",
    "# Now you can fit the model\n",
    "# history = model.fit(X_train, Y_oh_train, validation_data=(X_val, Y_oh_val), batch_size=64, epochs=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_loss = keras_cv.losses.FocalLoss(alpha=0.25, gamma=2.0)\n",
    "\n",
    "model.compile(optimizer='adam', loss=focal_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# training the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_oh_train, validation_data\u001b[38;5;241m=\u001b[39m(X_val, Y_oh_val), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "history = model.fit(X_train, Y_oh_train, validation_data=(X_val, Y_oh_val), batch_size=64, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-04 20:44:49.204762: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: logits and labels must have the same first dimension, got logits shape [32,3] and labels shape [96]\n",
      "\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/anaconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"/opt/anaconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/anaconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n      self._run_once()\n    File \"/opt/anaconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n      handle._run()\n    File \"/opt/anaconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n      result = self._run_cell(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n      result = runner(coro)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/vp/gk62g5t141b4ppmfkv7wmxl40000gn/T/ipykernel_64036/2214795209.py\", line 1, in <module>\n      loss = model.evaluate(X_test, Y_oh_test, verbose=0)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 2072, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1852, in test_function\n      return step_function(self, iterator)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1836, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1824, in run_step\n      outputs = model.test_step(data)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1790, in test_step\n      self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/losses.py\", line 2078, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/backend.py\", line 5660, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [32,3] and labels shape [96]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_test_function_1008]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, Y_oh_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest loss (cross-entropy and accuracy):\u001b[39m\u001b[38;5;124m'\u001b[39m,loss)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/anaconda3/lib/python3.11/site-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 701, in start\n      self.io_loop.start()\n    File \"/opt/anaconda3/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/anaconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n      self._run_once()\n    File \"/opt/anaconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n      handle._run()\n    File \"/opt/anaconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n      await self.process_one()\n    File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n      await dispatch(*args)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n      await result\n    File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n      reply_content = await reply_content\n    File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n      res = shell.run_cell(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n      result = self._run_cell(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n      result = runner(coro)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/vp/gk62g5t141b4ppmfkv7wmxl40000gn/T/ipykernel_64036/2214795209.py\", line 1, in <module>\n      loss = model.evaluate(X_test, Y_oh_test, verbose=0)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 2072, in evaluate\n      tmp_logs = self.test_function(iterator)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1852, in test_function\n      return step_function(self, iterator)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1836, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1824, in run_step\n      outputs = model.test_step(data)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1790, in test_step\n      self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/losses.py\", line 2078, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/backend.py\", line 5660, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nlogits and labels must have the same first dimension, got logits shape [32,3] and labels shape [96]\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_test_function_1008]"
     ]
    }
   ],
   "source": [
    "loss = model.evaluate(X_test, Y_oh_test, verbose=0)\n",
    "print('Test loss (cross-entropy and accuracy):',loss)\n",
    "print()\n",
    "W = model.get_weights()\n",
    "for ii in range(len(W)//2):\n",
    "    print(\"Layer %d\" %ii)\n",
    "    print('Bias:\\n', W[2*ii + 1])\n",
    "    print('W:\\n', W[2*ii])\n",
    "    print()\n",
    "\n",
    "plt.plot(history.history['loss'], label = \"Train loss\")\n",
    "plt.plot(history.history['val_loss'], label = \"Val loss\")\n",
    "plt.xlabel(\"Epoch (iteration)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(history.history['accuracy'], label = \"Train accuarcy\")\n",
    "plt.plot(history.history['val_accuracy'], label = \"Val accuarcy\")\n",
    "plt.xlabel(\"Epoch (iteration)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References for creating this jupyter notebook \n",
    "\n",
    "1) https://keras.io/guides/functional_api/\n",
    "\n",
    "2) https://keras.io/api/models/sequential/\n",
    "\n",
    "3) https://keras.io/api/models/\n",
    "\n",
    "4) https://towardsdatascience.com/3-ways-to-create-a-machine-learning-model-with-keras-and-tensorflow-2-0-de09323af4d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
