{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Approaches to Defining Neural Networks with Keras and TensorFlow\n",
    "\n",
    "[TensorFlow](https://www.tensorflow.org/) 2.0 came with many new exciting updates. One of these updates was full integration with the very popular [Keras API](https://keras.io/) for developing deep learning models. Before TensorFlow 2.0, you had two install TensorfFlow and Keras separately. Now, Keras comes as a submodule of TensorFlow (*i.e.*, tensorflow.keras). We will be using Keras and TensorFlow on the majority of tutorials in this class. There are 3 ways to define Neural Networks with Keras. In this tutorial we will cover these different ways.\n",
    "\n",
    "The learning goals of this tutorial are:\n",
    "    - Introduce the Keras sequential API, functional API and model subclassing methods for defining neural networks;\n",
    "    - Illustrate a simple classiifcation problem using the Iris dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Python Libraries \n",
    "\n",
    "If you get an error that a library is not installed, most libraries you can stall on a jupyter notebook by creating a new cell and typing:\n",
    "\n",
    "- *! pip install library_name*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical # Function to convert labels to one-hot encoding\n",
    "import pandas as pd  \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.datasets import load_iris  # Function for loading the Iris dataset\n",
    "from sklearn.model_selection import train_test_split # Function for splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and return to the defined variable \n",
    "dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris data into a DataFrame\n",
    "dframe = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "# add \"target_label\" column to the dataset and name it \"label\"\n",
    "dframe['labels'] = dataset.target.astype(int) # Labels are represented as integers\n",
    "# use of String label\n",
    "dframe['label_names'] = dframe.labels.replace(dict(enumerate(dataset.target_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>labels</th>\n",
       "      <th>label_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   labels label_names  \n",
       "0       0      setosa  \n",
       "1       0      setosa  \n",
       "2       0      setosa  \n",
       "3       0      setosa  \n",
       "4       0      setosa  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prints the 5 first rows/samples of the dataset\n",
    "dframe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)      labels  \n",
       "count        150.000000  150.000000  \n",
       "mean           1.199333    1.000000  \n",
       "std            0.762238    0.819232  \n",
       "min            0.100000    0.000000  \n",
       "25%            0.300000    0.000000  \n",
       "50%            1.300000    1.000000  \n",
       "75%            1.800000    2.000000  \n",
       "max            2.500000    2.000000  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generates a short description of the dataset (missing values, mean values, etc.)\n",
    "dframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation and Test Sets Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the features and labels from the dataset \n",
    "X = np.asarray(dframe[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']])\n",
    "Y = np.asarray(dframe['labels'])\n",
    "\n",
    "# First we will shuffle the samples\n",
    "indexes = np.arange(X.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "X = X[indexes,:]\n",
    "Y = Y[indexes]\n",
    "\n",
    "# Then, we split our data into train/val/test sets\n",
    "train_split = int(0.5*Y.size)\n",
    "val_split = int(0.75*Y.size)\n",
    "\n",
    "X_train = X[:train_split,:]\n",
    "Y_train = Y[:train_split]\n",
    "\n",
    "X_val = X[train_split:val_split,:]\n",
    "Y_val = Y[train_split:val_split]\n",
    "\n",
    "X_test = X[val_split:,:]\n",
    "Y_test = Y[val_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min-max data normalization\n",
    "x_train_min = X_train.min(axis = 0, keepdims = True)\n",
    "x_train_max = X_train.max(axis = 0, keepdims = True)\n",
    "\n",
    "X_train = (X_train - x_train_min)/(x_train_max - x_train_min)\n",
    "X_val = (X_val - x_train_min)/(x_train_max - x_train_min)\n",
    "X_test = (X_test - x_train_min)/(x_train_max - x_train_min)\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "#Activity suggestion:\n",
    "# 1. Change the min-max normalization above by standardization ((X - mean)/(std))\n",
    "# 2. Don't normalize the data and see what happens\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing Labels using one-hot-ecoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train[:5]:\n",
      "[2 0 1 2 0]\n",
      "\n",
      "Y_oh_train[:5]=\n",
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "k = np.unique(Y).size\n",
    "Y_oh_train = to_categorical(Y_train, k) \n",
    "Y_oh_val = to_categorical(Y_val, k) \n",
    "Y_oh_test = to_categorical(Y_test, k)\n",
    "# Displaying the 5 first elemnts\n",
    "print('Y_train[:5]:')\n",
    "print(Y_train[:5])\n",
    "print('\\nY_oh_train[:5]=')\n",
    "print(Y_oh_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X_train :  (75, 4)\n",
      "Size of X_val :  (37, 4)\n",
      "Size of X_test :  (38, 4)\n"
     ]
    }
   ],
   "source": [
    "print( \"Size of X_train : \" , X_train.shape)\n",
    "print( \"Size of X_val : \" , X_val.shape)\n",
    "print( \"Size of X_test : \" , X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Approaches for Defining Neural Networks\n",
    "\n",
    "### 1. The Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Dense)              (None, 5)                 25        \n",
      "                                                                 \n",
      " layer2 (Dense)              (None, 10)                60        \n",
      "                                                                 \n",
      " layer3 (Dense)              (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118\n",
      "Trainable params: 118\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "# Passing a list of layers to the constructor\n",
    "model = Sequential([\n",
    "    tf.keras.layers.Dense(5, activation='relu', input_shape=(4,) , name = \"layer1\"),\n",
    "    tf.keras.layers.Dense(10, activation='relu' , name = \"layer2\"),\n",
    "    tf.keras.layers.Dense(3, activation='softmax', name = \"layer3\"),\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 5)                 25        \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 10)                60        \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118\n",
      "Trainable params: 118\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "# This returns a tensor\n",
    "input_tensor = tf.keras.layers.Input(shape=(4,))\n",
    "# A layer instance is callable on a tensor, and returns a tensor\n",
    "x1 = tf.keras.layers.Dense(5, activation='relu')(input_tensor)\n",
    "x2 = tf.keras.layers.Dense(10, activation='relu')(x1)\n",
    "out_tensor = tf.keras.layers.Dense(3, activation='softmax')(x2)\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model = Model(inputs=input_tensor, outputs=out_tensor)\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Subclassing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_neural_network_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_15 (Dense)            multiple                  25        \n",
      "                                                                 \n",
      " dense_16 (Dense)            multiple                  60        \n",
      "                                                                 \n",
      " dense_17 (Dense)            multiple                  33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118\n",
      "Trainable params: 118\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class MyNeuralNetwork(Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyNeuralNetwork, self).__init__(**kwargs)\n",
    "        self.dense1 = tf.keras.layers.Dense(5, activation='relu', )\n",
    "        self.dense2 = tf.keras.layers.Dense(10, activation='relu')\n",
    "        self.dense3 = tf.keras.layers.Dense(3, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x1 = self.dense1(inputs)\n",
    "        x2 = self.dense2(x1)\n",
    "        out_tensor = self.dense3(x2)\n",
    "        return out_tensor\n",
    "model = MyNeuralNetwork()\n",
    "model.build(input_shape = (None,4))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Define the focal loss function\n",
    "class CategoricalFocalCrossentropy(tf.keras.losses.Loss):\n",
    "    def __init__(self, gamma=2.0, alpha=0.25, name='categorical_focal_loss'):\n",
    "        super(CategoricalFocalCrossentropy, self).__init__(name=name)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Clip the predictions to prevent log(0) error\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "\n",
    "        # Calculate cross entropy\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "        \n",
    "        # Calculate focal loss\n",
    "        loss = self.alpha * K.pow(1 - y_pred, self.gamma) * cross_entropy\n",
    "        return K.sum(loss, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Loss.__call__() got an unexpected keyword argument 'gamma'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m focal_loss \u001b[38;5;241m=\u001b[39m CategoricalFocalCrossentropy(gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39mfocal_loss(gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: Loss.__call__() got an unexpected keyword argument 'gamma'"
     ]
    }
   ],
   "source": [
    "from tensorflow_addons.losses import CategoricalFocalCrossentropy\n",
    "focal_loss = CategoricalFocalCrossentropy(gamma=2.0, alpha=0.25)\n",
    "model.compile(optimizer='adam', loss=focal_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 240, in __call__\n        self.build(y_pred)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 182, in build\n        self._losses = tf.nest.map_structure(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 353, in _get_loss_object\n        loss = losses_mod.get(loss)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/losses.py\", line 2653, in get\n        return deserialize(identifier, use_legacy_format=use_legacy_format)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/losses.py\", line 2600, in deserialize\n        return legacy_serialization.deserialize_keras_object(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/saving/legacy/serialization.py\", line 543, in deserialize_keras_object\n        raise ValueError(\n\n    ValueError: Unknown loss function: 'focal_loss'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# training the model \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, Y_oh_train, validation_data\u001b[38;5;241m=\u001b[39m(X_val,Y_oh_val),batch_size\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/vp/gk62g5t141b4ppmfkv7wmxl40000gn/T/__autograph_generated_file2v3dczca.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 240, in __call__\n        self.build(y_pred)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 182, in build\n        self._losses = tf.nest.map_structure(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 353, in _get_loss_object\n        loss = losses_mod.get(loss)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/losses.py\", line 2653, in get\n        return deserialize(identifier, use_legacy_format=use_legacy_format)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/losses.py\", line 2600, in deserialize\n        return legacy_serialization.deserialize_keras_object(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/saving/legacy/serialization.py\", line 543, in deserialize_keras_object\n        raise ValueError(\n\n    ValueError: Unknown loss function: 'focal_loss'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n"
     ]
    }
   ],
   "source": [
    "# training the model \n",
    "history = model.fit(X_train, Y_oh_train, validation_data=(X_val,Y_oh_val),batch_size= 64, epochs= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1790, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 240, in __call__\n        self.build(y_pred)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 182, in build\n        self._losses = tf.nest.map_structure(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 353, in _get_loss_object\n        loss = losses_mod.get(loss)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/losses.py\", line 2653, in get\n        return deserialize(identifier, use_legacy_format=use_legacy_format)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/losses.py\", line 2600, in deserialize\n        return legacy_serialization.deserialize_keras_object(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/saving/legacy/serialization.py\", line 543, in deserialize_keras_object\n        raise ValueError(\n\n    ValueError: Unknown loss function: 'CategoricalFocalCrossentropy'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, Y_oh_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest loss (cross-entropy and accuracy):\u001b[39m\u001b[38;5;124m'\u001b[39m,loss)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/vp/gk62g5t141b4ppmfkv7wmxl40000gn/T/__autograph_generated_filez_lnbsi7.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1852, in test_function  *\n        return step_function(self, iterator)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1836, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1824, in run_step  **\n        outputs = model.test_step(data)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1790, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 240, in __call__\n        self.build(y_pred)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 182, in build\n        self._losses = tf.nest.map_structure(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/engine/compile_utils.py\", line 353, in _get_loss_object\n        loss = losses_mod.get(loss)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/losses.py\", line 2653, in get\n        return deserialize(identifier, use_legacy_format=use_legacy_format)\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/losses.py\", line 2600, in deserialize\n        return legacy_serialization.deserialize_keras_object(\n    File \"/opt/anaconda3/lib/python3.11/site-packages/keras/saving/legacy/serialization.py\", line 543, in deserialize_keras_object\n        raise ValueError(\n\n    ValueError: Unknown loss function: 'CategoricalFocalCrossentropy'. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.\n"
     ]
    }
   ],
   "source": [
    "loss = model.evaluate(X_test, Y_oh_test, verbose=0)\n",
    "print('Test loss (cross-entropy and accuracy):',loss)\n",
    "print()\n",
    "W = model.get_weights()\n",
    "for ii in range(len(W)//2):\n",
    "    print(\"Layer %d\" %ii)\n",
    "    print('Bias:\\n', W[2*ii + 1])\n",
    "    print('W:\\n', W[2*ii])\n",
    "    print()\n",
    "\n",
    "plt.plot(history.history['loss'], label = \"Train loss\")\n",
    "plt.plot(history.history['val_loss'], label = \"Val loss\")\n",
    "plt.xlabel(\"Epoch (iteration)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(history.history['accuracy'], label = \"Train accuarcy\")\n",
    "plt.plot(history.history['val_accuracy'], label = \"Val accuarcy\")\n",
    "plt.xlabel(\"Epoch (iteration)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References for creating this jupyter notebook \n",
    "\n",
    "1) https://keras.io/guides/functional_api/\n",
    "\n",
    "2) https://keras.io/api/models/sequential/\n",
    "\n",
    "3) https://keras.io/api/models/\n",
    "\n",
    "4) https://towardsdatascience.com/3-ways-to-create-a-machine-learning-model-with-keras-and-tensorflow-2-0-de09323af4d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
